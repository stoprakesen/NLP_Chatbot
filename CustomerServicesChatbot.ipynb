{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9gUaOSSsZkXO",
    "outputId": "da317f8d-d3e2-40bb-ac96-7117587e81a1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/secilsair/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/secilsair/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "F64Wh2_jbw3y",
    "tags": []
   },
   "outputs": [],
   "source": [
    "words=[]\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']\n",
    "data_file = open('intents.json').read()\n",
    "intents = json.loads(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "1C5FsW-Zb088",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "\n",
    "        #tokenize each word\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        #add documents in the corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FK5NDI55clOe",
    "outputId": "4a57ed79-ac0a-478f-d8fd-585f00e3ed5c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471 documents\n",
      "46 classes ['About_Delizeroo', 'Fees_on_Delizeroo', 'Inviting_friends', 'about_my_order', 'admission', 'cancel_order', 'canteen', 'college intake', 'committee', 'computerhod', 'course', 'creator', 'delizeroo_editions', 'document', 'event', 'extchod', 'facilities', 'fees', 'floors', 'goodbye', 'greeting', 'hod', 'hostel', 'hours', 'infrastructure', 'ithod', 'library', 'location', 'menu', 'name', 'number', 'order_information', 'placement', 'principal', 'ragging', 'random', 'salutaion', 'scholarship', 'sem', 'sports', 'swear', 'syllabus', 'task', 'thanks', 'uniform', 'vacation']\n",
      "304 unique lemmatized words [\"'m\", \"'s\", '(', ')', ',', '.', 'a', 'about', 'ac', 'accept', 'active', 'activity', 'add', 'address', 'admision', 'admission', 'against', 'ai/ml', 'allotment', 'am', 'an', 'and', 'antiragging', 'any', 'anyone', 'are', 'around', 'arrives', 'as', 'asshole', 'at', 'attend', 'automobile', 'available', 'average', 'awesome', 'be', 'between', 'big', 'bitch', 'book', 'boy', 'branch', 'bring', 'building', 'by', 'bye', 'cafetaria', 'call', 'called', 'campus', 'can', 'cancel', 'canteen', 'capacity', 'case', 'cash', 'casuals', 'ce', 'chatting', 'chemical', 'civil', 'code', 'college', 'come', 'committe', 'committee', 'comp', 'company', 'computer', 'conducted', 'contact', 'course', 'create', 'created', 'creator', 'credit', 'cya', 'date', 'day', 'deliveroo', 'delizeroo', 'designed', 'detail', 'developer', 'different', 'distance', 'do', 'document', 'doe', 'done', 'dress', 'dresscode', 'dumb', 'during', 'each', 'eat', 'edition', 'end', 'engineering', 'event', 'exam', 'extc', 'facility', 'far', 'fee', 'first', 'floor', 'food', 'for', 'fourth', 'free', 'from', 'fuck', 'fucker', 'function', 'game', 'get', 'girl', 'give', 'go', 'good', 'goodbye', 'got', 'gtg', 'guy', 'have', 'held', 'hell', 'hello', 'help', 'helpful', 'helping', 'here', 'hey', 'heyy', 'hi', 'history', 'hod', 'hola', 'holiday', 'hostel', 'hour', 'how', 'i', 'idiot', 'if', 'in', 'incident', 'info', 'information', 'infrastructure', 'intake', 'is', 'it', 'job', 'junior', 'k', 'late', 'later', 'leaving', 'lecture', 'library', 'like', 'list', 'located', 'location', 'long', 'love', 'm', 'made', 'many', 'marry', 'max', 'maximum', 'me', 'mechanical', 'menu', 'minimum', 'more', 'much', 'my', \"n't\", 'name', 'need', 'needed', 'new', 'next', 'nice', 'no', 'non-ac', 'not', 'number', 'of', 'offer', 'offered', 'office', 'ok', 'okay', 'okie', 'okk', 'on', 'open', 'operation', 'order', 'organised', 'package', 'per', 'phone', 'placement', 'please', 'practice', 'principal', 'process', 'provide', 'provided', 'ragging', 'reach', 'receive', 'recruitment', 'required', 'rider', 'room', 'saturday', 'schedule', 'scholarship', 'seat', 'second', 'see', 'sem', 'semester', 'servive', 'should', 'shut', 'sign', 'size', 'something', 'spend', 'sport', 'start', 'student', 'stundent', 'stupid', 'support', 'syllabus', 'take', 'taken', 'taking', 'talk', 'tall', 'technology', 'telephone', 'tell', 'thank', 'thanks', 'that', 'the', 'there', 'these', 'thing', 'third', 'this', 'till', 'time', 'timetable', 'timing', 'to', 'ttyl', 'u', 'uni', 'uniform', 'univrsity', 'up', 'use', 'user', 'vacation', 'variety', 'visit', 'want', 'we', 'wear', 'well', 'what', 'whats', 'whatsup', 'whatv', 'when', 'where', 'wheres', 'which', 'who', 'whom', 'why', 'will', 'with', 'work', 'working', 'would', 'wrong', 'ya', 'year', 'you', 'your', '’']\n"
     ]
    }
   ],
   "source": [
    "# lemmaztize and lower each word and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "# sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "# documents = combination between patterns and intents\n",
    "print (len(documents), \"documents\")\n",
    "# classes = intents\n",
    "print (len(classes), \"classes\", classes)\n",
    "# words = all words, vocabulary\n",
    "print (len(words), \"unique lemmatized words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "LkKi1JbCcwPc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(words,open('unique_words.pkl','wb'))\n",
    "pickle.dump(classes,open('classes1.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "i_SV4Cq1c04F",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create training data\n",
    "training = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # lemmatize each word - create base word, in attempt to represent related words\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array with 1, if word match found in current pattern\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "\n",
    "# Özellikleri ve etiketleri 'training' listesinden ayırma\n",
    "X = [item[0] for item in training]  # features\n",
    "y = [item[1] for item in training]  # label\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "\n",
    "\n",
    "# Numpy dizisine dönüştürme\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "PNSz1gaDeo0p",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n",
    "# equal to number of intents to predict output intent with softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(X_train[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(y_train[0]), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "xoEhAC0eevGr",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = Precision()\n",
    "        self.recall = Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()\n",
    "\n",
    "\n",
    "\n",
    "sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, \n",
    "              metrics=['accuracy', Precision(), Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "KwbK5XIvm0mn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#hist = model.fit(np.array(train_x), np.array(train_y), epochs=100, batch_size=4, verbose=1)\n",
    "#model.save('my_model.h5', hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uTt3KeDIf2f5",
    "outputId": "84c62268-215d-4d66-95a5-2ba477b82090",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "63/63 [==============================] - 0s 402us/step - loss: 3.8170 - accuracy: 0.0186 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00\n",
      "Epoch 2/200\n",
      "63/63 [==============================] - 0s 343us/step - loss: 3.6967 - accuracy: 0.0771 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00\n",
      "Epoch 3/200\n",
      "63/63 [==============================] - 0s 331us/step - loss: 3.5422 - accuracy: 0.1170 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00\n",
      "Epoch 4/200\n",
      "63/63 [==============================] - 0s 336us/step - loss: 3.4077 - accuracy: 0.1596 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00\n",
      "Epoch 5/200\n",
      "63/63 [==============================] - 0s 339us/step - loss: 3.1765 - accuracy: 0.2101 - precision_1: 0.8333 - recall_1: 0.0133\n",
      "Epoch 6/200\n",
      "63/63 [==============================] - 0s 342us/step - loss: 2.8828 - accuracy: 0.2819 - precision_1: 0.9429 - recall_1: 0.0878  \n",
      "Epoch 7/200\n",
      "63/63 [==============================] - 0s 339us/step - loss: 2.7004 - accuracy: 0.3032 - precision_1: 0.9200 - recall_1: 0.1223  \n",
      "Epoch 8/200\n",
      "63/63 [==============================] - 0s 355us/step - loss: 2.5589 - accuracy: 0.3431 - precision_1: 0.8730 - recall_1: 0.1463\n",
      "Epoch 9/200\n",
      "63/63 [==============================] - 0s 359us/step - loss: 2.3014 - accuracy: 0.3989 - precision_1: 0.9254 - recall_1: 0.1649\n",
      "Epoch 10/200\n",
      "63/63 [==============================] - 0s 349us/step - loss: 2.1598 - accuracy: 0.4309 - precision_1: 0.9222 - recall_1: 0.2207\n",
      "Epoch 11/200\n",
      "63/63 [==============================] - 0s 354us/step - loss: 1.9479 - accuracy: 0.4947 - precision_1: 0.9490 - recall_1: 0.2473\n",
      "Epoch 12/200\n",
      "63/63 [==============================] - 0s 361us/step - loss: 1.8257 - accuracy: 0.5080 - precision_1: 0.8966 - recall_1: 0.2766\n",
      "Epoch 13/200\n",
      "63/63 [==============================] - 0s 349us/step - loss: 1.8522 - accuracy: 0.5213 - precision_1: 0.8125 - recall_1: 0.2766\n",
      "Epoch 14/200\n",
      "63/63 [==============================] - 0s 344us/step - loss: 1.5801 - accuracy: 0.5638 - precision_1: 0.8986 - recall_1: 0.3537\n",
      "Epoch 15/200\n",
      "63/63 [==============================] - 0s 355us/step - loss: 1.5182 - accuracy: 0.5771 - precision_1: 0.8735 - recall_1: 0.3856\n",
      "Epoch 16/200\n",
      "63/63 [==============================] - 0s 360us/step - loss: 1.3083 - accuracy: 0.6250 - precision_1: 0.8827 - recall_1: 0.4601\n",
      "Epoch 17/200\n",
      "63/63 [==============================] - 0s 338us/step - loss: 1.3636 - accuracy: 0.6144 - precision_1: 0.8824 - recall_1: 0.4388\n",
      "Epoch 18/200\n",
      "63/63 [==============================] - 0s 612us/step - loss: 1.2876 - accuracy: 0.6596 - precision_1: 0.9344 - recall_1: 0.4548\n",
      "Epoch 19/200\n",
      "63/63 [==============================] - 0s 340us/step - loss: 1.1999 - accuracy: 0.6356 - precision_1: 0.8872 - recall_1: 0.4601\n",
      "Epoch 20/200\n",
      "63/63 [==============================] - 0s 354us/step - loss: 1.0676 - accuracy: 0.6729 - precision_1: 0.8761 - recall_1: 0.5080\n",
      "Epoch 21/200\n",
      "63/63 [==============================] - 0s 351us/step - loss: 0.9863 - accuracy: 0.7074 - precision_1: 0.8970 - recall_1: 0.5559\n",
      "Epoch 22/200\n",
      "63/63 [==============================] - 0s 340us/step - loss: 0.9444 - accuracy: 0.7500 - precision_1: 0.9160 - recall_1: 0.5798\n",
      "Epoch 23/200\n",
      "63/63 [==============================] - 0s 349us/step - loss: 0.8898 - accuracy: 0.7394 - precision_1: 0.9076 - recall_1: 0.5745\n",
      "Epoch 24/200\n",
      "63/63 [==============================] - 0s 352us/step - loss: 0.9585 - accuracy: 0.7101 - precision_1: 0.9099 - recall_1: 0.5638\n",
      "Epoch 25/200\n",
      "63/63 [==============================] - 0s 338us/step - loss: 0.7982 - accuracy: 0.7819 - precision_1: 0.9012 - recall_1: 0.6064\n",
      "Epoch 26/200\n",
      "63/63 [==============================] - 0s 345us/step - loss: 0.7302 - accuracy: 0.7872 - precision_1: 0.8885 - recall_1: 0.6356\n",
      "Epoch 27/200\n",
      "63/63 [==============================] - 0s 344us/step - loss: 0.8195 - accuracy: 0.7633 - precision_1: 0.8927 - recall_1: 0.6197\n",
      "Epoch 28/200\n",
      "63/63 [==============================] - 0s 332us/step - loss: 0.6801 - accuracy: 0.7952 - precision_1: 0.9071 - recall_1: 0.6755\n",
      "Epoch 29/200\n",
      "63/63 [==============================] - 0s 345us/step - loss: 0.6951 - accuracy: 0.7846 - precision_1: 0.8763 - recall_1: 0.6596\n",
      "Epoch 30/200\n",
      "63/63 [==============================] - 0s 342us/step - loss: 0.6978 - accuracy: 0.7713 - precision_1: 0.9104 - recall_1: 0.6755\n",
      "Epoch 31/200\n",
      "63/63 [==============================] - 0s 336us/step - loss: 0.6415 - accuracy: 0.8085 - precision_1: 0.8919 - recall_1: 0.7021\n",
      "Epoch 32/200\n",
      "63/63 [==============================] - 0s 344us/step - loss: 0.6088 - accuracy: 0.8191 - precision_1: 0.9000 - recall_1: 0.7181\n",
      "Epoch 33/200\n",
      "63/63 [==============================] - 0s 343us/step - loss: 0.6179 - accuracy: 0.8271 - precision_1: 0.9308 - recall_1: 0.7154\n",
      "Epoch 34/200\n",
      "63/63 [==============================] - 0s 341us/step - loss: 0.5926 - accuracy: 0.8378 - precision_1: 0.9347 - recall_1: 0.7234\n",
      "Epoch 35/200\n",
      "63/63 [==============================] - 0s 335us/step - loss: 0.5473 - accuracy: 0.8511 - precision_1: 0.9298 - recall_1: 0.7394\n",
      "Epoch 36/200\n",
      "63/63 [==============================] - 0s 344us/step - loss: 0.5149 - accuracy: 0.8431 - precision_1: 0.9290 - recall_1: 0.7660\n",
      "Epoch 37/200\n",
      "63/63 [==============================] - 0s 346us/step - loss: 0.4939 - accuracy: 0.8484 - precision_1: 0.9299 - recall_1: 0.7766\n",
      "Epoch 38/200\n",
      "63/63 [==============================] - 0s 336us/step - loss: 0.5346 - accuracy: 0.8511 - precision_1: 0.8991 - recall_1: 0.7580\n",
      "Epoch 39/200\n",
      "63/63 [==============================] - 0s 351us/step - loss: 0.4651 - accuracy: 0.8537 - precision_1: 0.9177 - recall_1: 0.7713\n",
      "Epoch 40/200\n",
      "63/63 [==============================] - 0s 341us/step - loss: 0.5242 - accuracy: 0.8378 - precision_1: 0.9216 - recall_1: 0.7500\n",
      "Epoch 41/200\n",
      "63/63 [==============================] - 0s 337us/step - loss: 0.5400 - accuracy: 0.8457 - precision_1: 0.9062 - recall_1: 0.7713\n",
      "Epoch 42/200\n",
      "63/63 [==============================] - 0s 345us/step - loss: 0.5479 - accuracy: 0.8564 - precision_1: 0.9213 - recall_1: 0.7473\n",
      "Epoch 43/200\n",
      "63/63 [==============================] - 0s 340us/step - loss: 0.5059 - accuracy: 0.8484 - precision_1: 0.9231 - recall_1: 0.7660\n",
      "Epoch 44/200\n",
      "63/63 [==============================] - 0s 336us/step - loss: 0.4866 - accuracy: 0.8378 - precision_1: 0.9068 - recall_1: 0.7500\n",
      "Epoch 45/200\n",
      "63/63 [==============================] - 0s 343us/step - loss: 0.3764 - accuracy: 0.8963 - precision_1: 0.9412 - recall_1: 0.8511\n",
      "Epoch 46/200\n",
      "63/63 [==============================] - 0s 337us/step - loss: 0.3715 - accuracy: 0.8989 - precision_1: 0.9275 - recall_1: 0.8165\n",
      "Epoch 47/200\n",
      "63/63 [==============================] - 0s 331us/step - loss: 0.3910 - accuracy: 0.8697 - precision_1: 0.9371 - recall_1: 0.8324\n",
      "Epoch 48/200\n",
      "63/63 [==============================] - 0s 342us/step - loss: 0.4342 - accuracy: 0.8670 - precision_1: 0.9215 - recall_1: 0.8112\n",
      "Epoch 49/200\n",
      "63/63 [==============================] - 0s 348us/step - loss: 0.3845 - accuracy: 0.8856 - precision_1: 0.9267 - recall_1: 0.8404\n",
      "Epoch 50/200\n",
      "63/63 [==============================] - 0s 332us/step - loss: 0.3493 - accuracy: 0.8989 - precision_1: 0.9459 - recall_1: 0.8378\n",
      "Epoch 51/200\n",
      "63/63 [==============================] - 0s 347us/step - loss: 0.3694 - accuracy: 0.8750 - precision_1: 0.9489 - recall_1: 0.8404\n",
      "Epoch 52/200\n",
      "63/63 [==============================] - 0s 350us/step - loss: 0.3609 - accuracy: 0.8883 - precision_1: 0.9349 - recall_1: 0.8404\n",
      "Epoch 53/200\n",
      "63/63 [==============================] - 0s 335us/step - loss: 0.3550 - accuracy: 0.8750 - precision_1: 0.9263 - recall_1: 0.8351\n",
      "Epoch 54/200\n",
      "63/63 [==============================] - 0s 341us/step - loss: 0.3515 - accuracy: 0.9069 - precision_1: 0.9561 - recall_1: 0.8697\n",
      "Epoch 55/200\n",
      "63/63 [==============================] - 0s 346us/step - loss: 0.4169 - accuracy: 0.8803 - precision_1: 0.9273 - recall_1: 0.8138\n",
      "Epoch 56/200\n",
      "63/63 [==============================] - 0s 343us/step - loss: 0.4027 - accuracy: 0.8830 - precision_1: 0.9249 - recall_1: 0.8191\n",
      "Epoch 57/200\n",
      "63/63 [==============================] - 0s 339us/step - loss: 0.3460 - accuracy: 0.9069 - precision_1: 0.9407 - recall_1: 0.8431\n",
      "Epoch 58/200\n",
      "63/63 [==============================] - 0s 344us/step - loss: 0.3427 - accuracy: 0.8989 - precision_1: 0.9475 - recall_1: 0.8644\n",
      "Epoch 59/200\n",
      "63/63 [==============================] - 0s 344us/step - loss: 0.3047 - accuracy: 0.9043 - precision_1: 0.9415 - recall_1: 0.8564\n",
      "Epoch 60/200\n",
      "63/63 [==============================] - 0s 341us/step - loss: 0.3386 - accuracy: 0.9122 - precision_1: 0.9506 - recall_1: 0.8697\n",
      "Epoch 61/200\n",
      "63/63 [==============================] - 0s 342us/step - loss: 0.2898 - accuracy: 0.9043 - precision_1: 0.9395 - recall_1: 0.8670\n",
      "Epoch 62/200\n",
      "63/63 [==============================] - 0s 341us/step - loss: 0.3167 - accuracy: 0.9069 - precision_1: 0.9558 - recall_1: 0.8617\n",
      "Epoch 63/200\n",
      "63/63 [==============================] - 0s 338us/step - loss: 0.3271 - accuracy: 0.8989 - precision_1: 0.9481 - recall_1: 0.8750\n",
      "Epoch 64/200\n",
      "63/63 [==============================] - 0s 344us/step - loss: 0.2403 - accuracy: 0.9255 - precision_1: 0.9600 - recall_1: 0.8936\n",
      "Epoch 65/200\n",
      "63/63 [==============================] - 0s 339us/step - loss: 0.2757 - accuracy: 0.9043 - precision_1: 0.9320 - recall_1: 0.8750\n",
      "Epoch 66/200\n",
      "63/63 [==============================] - 0s 639us/step - loss: 0.2750 - accuracy: 0.8989 - precision_1: 0.9335 - recall_1: 0.8590\n",
      "Epoch 67/200\n",
      "63/63 [==============================] - 0s 354us/step - loss: 0.2838 - accuracy: 0.9096 - precision_1: 0.9506 - recall_1: 0.8697\n",
      "Epoch 68/200\n",
      "63/63 [==============================] - 0s 340us/step - loss: 0.3479 - accuracy: 0.9043 - precision_1: 0.9181 - recall_1: 0.8644\n",
      "Epoch 69/200\n",
      "63/63 [==============================] - 0s 353us/step - loss: 0.3210 - accuracy: 0.8963 - precision_1: 0.9329 - recall_1: 0.8511\n",
      "Epoch 70/200\n",
      "63/63 [==============================] - 0s 350us/step - loss: 0.3188 - accuracy: 0.9043 - precision_1: 0.9497 - recall_1: 0.8537\n",
      "Epoch 71/200\n",
      "63/63 [==============================] - 0s 336us/step - loss: 0.3295 - accuracy: 0.9069 - precision_1: 0.9314 - recall_1: 0.8670\n",
      "Epoch 72/200\n",
      "63/63 [==============================] - 0s 352us/step - loss: 0.2749 - accuracy: 0.9202 - precision_1: 0.9432 - recall_1: 0.8830\n",
      "Epoch 73/200\n",
      "63/63 [==============================] - 0s 342us/step - loss: 0.2646 - accuracy: 0.9255 - precision_1: 0.9548 - recall_1: 0.8989\n",
      "Epoch 74/200\n",
      "63/63 [==============================] - 0s 334us/step - loss: 0.2803 - accuracy: 0.9202 - precision_1: 0.9535 - recall_1: 0.8723\n",
      "Epoch 75/200\n",
      "63/63 [==============================] - 0s 346us/step - loss: 0.2719 - accuracy: 0.9202 - precision_1: 0.9654 - recall_1: 0.8910\n",
      "Epoch 76/200\n",
      "63/63 [==============================] - 0s 344us/step - loss: 0.2671 - accuracy: 0.9043 - precision_1: 0.9452 - recall_1: 0.8723\n",
      "Epoch 77/200\n",
      "63/63 [==============================] - 0s 442us/step - loss: 0.2589 - accuracy: 0.9176 - precision_1: 0.9539 - recall_1: 0.8803\n",
      "Epoch 78/200\n",
      "63/63 [==============================] - 0s 390us/step - loss: 0.2571 - accuracy: 0.9255 - precision_1: 0.9433 - recall_1: 0.8856\n",
      "Epoch 79/200\n",
      "63/63 [==============================] - 0s 340us/step - loss: 0.2810 - accuracy: 0.9176 - precision_1: 0.9540 - recall_1: 0.8830\n",
      "Epoch 80/200\n",
      "63/63 [==============================] - 0s 351us/step - loss: 0.2945 - accuracy: 0.9043 - precision_1: 0.9452 - recall_1: 0.8723\n",
      "Epoch 81/200\n",
      "63/63 [==============================] - 0s 355us/step - loss: 0.1980 - accuracy: 0.9362 - precision_1: 0.9661 - recall_1: 0.9096\n",
      "Epoch 82/200\n",
      "63/63 [==============================] - 0s 345us/step - loss: 0.2230 - accuracy: 0.9309 - precision_1: 0.9688 - recall_1: 0.9069\n",
      "Epoch 83/200\n",
      "63/63 [==============================] - 0s 344us/step - loss: 0.2433 - accuracy: 0.9176 - precision_1: 0.9492 - recall_1: 0.8936\n",
      "Epoch 84/200\n",
      "63/63 [==============================] - 0s 344us/step - loss: 0.2214 - accuracy: 0.9282 - precision_1: 0.9552 - recall_1: 0.9069\n",
      "Epoch 85/200\n",
      "63/63 [==============================] - 0s 343us/step - loss: 0.2507 - accuracy: 0.9122 - precision_1: 0.9412 - recall_1: 0.8936\n",
      "Epoch 86/200\n",
      "63/63 [==============================] - 0s 336us/step - loss: 0.2293 - accuracy: 0.9176 - precision_1: 0.9350 - recall_1: 0.8803\n",
      "Epoch 87/200\n",
      "63/63 [==============================] - 0s 350us/step - loss: 0.2575 - accuracy: 0.9096 - precision_1: 0.9272 - recall_1: 0.8803\n",
      "Epoch 88/200\n",
      "63/63 [==============================] - 0s 334us/step - loss: 0.2858 - accuracy: 0.9176 - precision_1: 0.9462 - recall_1: 0.8883\n",
      "Epoch 89/200\n",
      "63/63 [==============================] - 0s 339us/step - loss: 0.2260 - accuracy: 0.9202 - precision_1: 0.9497 - recall_1: 0.9043\n",
      "Epoch 90/200\n",
      "63/63 [==============================] - 0s 345us/step - loss: 0.2052 - accuracy: 0.9362 - precision_1: 0.9636 - recall_1: 0.9149\n",
      "Epoch 91/200\n",
      "63/63 [==============================] - 0s 341us/step - loss: 0.2233 - accuracy: 0.9335 - precision_1: 0.9438 - recall_1: 0.8936\n",
      "Epoch 92/200\n",
      "63/63 [==============================] - 0s 337us/step - loss: 0.1999 - accuracy: 0.9468 - precision_1: 0.9609 - recall_1: 0.9149\n",
      "Epoch 93/200\n",
      "63/63 [==============================] - 0s 349us/step - loss: 0.2490 - accuracy: 0.9255 - precision_1: 0.9496 - recall_1: 0.9016\n",
      "Epoch 94/200\n",
      "63/63 [==============================] - 0s 343us/step - loss: 0.2297 - accuracy: 0.9229 - precision_1: 0.9469 - recall_1: 0.9016\n",
      "Epoch 95/200\n",
      "63/63 [==============================] - 0s 343us/step - loss: 0.1982 - accuracy: 0.9441 - precision_1: 0.9607 - recall_1: 0.9096\n",
      "Epoch 96/200\n",
      "63/63 [==============================] - 0s 384us/step - loss: 0.2921 - accuracy: 0.9043 - precision_1: 0.9268 - recall_1: 0.8750\n",
      "Epoch 97/200\n",
      "63/63 [==============================] - 0s 366us/step - loss: 0.3186 - accuracy: 0.8963 - precision_1: 0.9222 - recall_1: 0.8511\n",
      "Epoch 98/200\n",
      "63/63 [==============================] - 0s 356us/step - loss: 0.2634 - accuracy: 0.9282 - precision_1: 0.9444 - recall_1: 0.9043\n",
      "Epoch 99/200\n",
      "63/63 [==============================] - 0s 343us/step - loss: 0.1630 - accuracy: 0.9521 - precision_1: 0.9664 - recall_1: 0.9176\n",
      "Epoch 100/200\n",
      "63/63 [==============================] - 0s 370us/step - loss: 0.2295 - accuracy: 0.9388 - precision_1: 0.9629 - recall_1: 0.8963\n",
      "Epoch 101/200\n",
      "63/63 [==============================] - 0s 354us/step - loss: 0.2110 - accuracy: 0.9388 - precision_1: 0.9637 - recall_1: 0.9176\n",
      "Epoch 102/200\n",
      "63/63 [==============================] - 0s 344us/step - loss: 0.2608 - accuracy: 0.9122 - precision_1: 0.9356 - recall_1: 0.8883\n",
      "Epoch 103/200\n",
      "63/63 [==============================] - 0s 373us/step - loss: 0.2323 - accuracy: 0.9282 - precision_1: 0.9632 - recall_1: 0.9043\n",
      "Epoch 104/200\n",
      "63/63 [==============================] - 0s 357us/step - loss: 0.1749 - accuracy: 0.9441 - precision_1: 0.9693 - recall_1: 0.9229\n",
      "Epoch 105/200\n",
      "63/63 [==============================] - 0s 354us/step - loss: 0.2244 - accuracy: 0.9388 - precision_1: 0.9579 - recall_1: 0.9069\n",
      "Epoch 106/200\n",
      "63/63 [==============================] - 0s 344us/step - loss: 0.2176 - accuracy: 0.9229 - precision_1: 0.9448 - recall_1: 0.9096\n",
      "Epoch 107/200\n",
      "63/63 [==============================] - 0s 355us/step - loss: 0.2258 - accuracy: 0.9309 - precision_1: 0.9551 - recall_1: 0.9043\n",
      "Epoch 108/200\n",
      "63/63 [==============================] - 0s 358us/step - loss: 0.2830 - accuracy: 0.9176 - precision_1: 0.9435 - recall_1: 0.8883\n",
      "Epoch 109/200\n",
      "63/63 [==============================] - 0s 339us/step - loss: 0.2676 - accuracy: 0.9309 - precision_1: 0.9530 - recall_1: 0.9176\n",
      "Epoch 110/200\n",
      "63/63 [==============================] - 0s 361us/step - loss: 0.2363 - accuracy: 0.9255 - precision_1: 0.9385 - recall_1: 0.8936\n",
      "Epoch 111/200\n",
      "63/63 [==============================] - 0s 350us/step - loss: 0.2641 - accuracy: 0.9096 - precision_1: 0.9385 - recall_1: 0.8936\n",
      "Epoch 112/200\n",
      "63/63 [==============================] - 0s 339us/step - loss: 0.2259 - accuracy: 0.9309 - precision_1: 0.9499 - recall_1: 0.9069\n",
      "Epoch 113/200\n",
      "63/63 [==============================] - 0s 347us/step - loss: 0.1662 - accuracy: 0.9628 - precision_1: 0.9720 - recall_1: 0.9229\n",
      "Epoch 114/200\n",
      "63/63 [==============================] - 0s 353us/step - loss: 0.1994 - accuracy: 0.9415 - precision_1: 0.9559 - recall_1: 0.9229\n",
      "Epoch 115/200\n",
      "63/63 [==============================] - 0s 348us/step - loss: 0.1848 - accuracy: 0.9388 - precision_1: 0.9582 - recall_1: 0.9149\n",
      "Epoch 116/200\n",
      "63/63 [==============================] - 0s 339us/step - loss: 0.2463 - accuracy: 0.9043 - precision_1: 0.9377 - recall_1: 0.8803\n",
      "Epoch 117/200\n",
      "63/63 [==============================] - 0s 350us/step - loss: 0.2428 - accuracy: 0.9362 - precision_1: 0.9497 - recall_1: 0.9043\n",
      "Epoch 118/200\n",
      "63/63 [==============================] - 0s 355us/step - loss: 0.1393 - accuracy: 0.9574 - precision_1: 0.9780 - recall_1: 0.9441\n",
      "Epoch 119/200\n",
      "63/63 [==============================] - 0s 338us/step - loss: 0.1636 - accuracy: 0.9415 - precision_1: 0.9564 - recall_1: 0.9335\n",
      "Epoch 120/200\n",
      "63/63 [==============================] - 0s 352us/step - loss: 0.1880 - accuracy: 0.9362 - precision_1: 0.9508 - recall_1: 0.9255\n",
      "Epoch 121/200\n",
      "63/63 [==============================] - 0s 353us/step - loss: 0.1502 - accuracy: 0.9495 - precision_1: 0.9751 - recall_1: 0.9388\n",
      "Epoch 122/200\n",
      "63/63 [==============================] - 0s 336us/step - loss: 0.2517 - accuracy: 0.9255 - precision_1: 0.9408 - recall_1: 0.8883\n",
      "Epoch 123/200\n",
      "63/63 [==============================] - 0s 353us/step - loss: 0.1843 - accuracy: 0.9388 - precision_1: 0.9560 - recall_1: 0.9255\n",
      "Epoch 124/200\n",
      "63/63 [==============================] - 0s 349us/step - loss: 0.1799 - accuracy: 0.9415 - precision_1: 0.9608 - recall_1: 0.9122\n",
      "Epoch 125/200\n",
      "63/63 [==============================] - 0s 357us/step - loss: 0.2430 - accuracy: 0.9441 - precision_1: 0.9660 - recall_1: 0.9069\n",
      "Epoch 126/200\n",
      "63/63 [==============================] - 0s 341us/step - loss: 0.1908 - accuracy: 0.9415 - precision_1: 0.9537 - recall_1: 0.9309\n",
      "Epoch 127/200\n",
      "63/63 [==============================] - 0s 350us/step - loss: 0.1600 - accuracy: 0.9521 - precision_1: 0.9644 - recall_1: 0.9362\n",
      "Epoch 128/200\n",
      "63/63 [==============================] - 0s 353us/step - loss: 0.2005 - accuracy: 0.9309 - precision_1: 0.9451 - recall_1: 0.9149\n",
      "Epoch 129/200\n",
      "63/63 [==============================] - 0s 339us/step - loss: 0.1678 - accuracy: 0.9415 - precision_1: 0.9484 - recall_1: 0.9282\n",
      "Epoch 130/200\n",
      "63/63 [==============================] - 0s 352us/step - loss: 0.1805 - accuracy: 0.9335 - precision_1: 0.9610 - recall_1: 0.9176\n",
      "Epoch 131/200\n",
      "63/63 [==============================] - 0s 591us/step - loss: 0.1971 - accuracy: 0.9335 - precision_1: 0.9505 - recall_1: 0.9202\n",
      "Epoch 132/200\n",
      "63/63 [==============================] - 0s 354us/step - loss: 0.1490 - accuracy: 0.9495 - precision_1: 0.9726 - recall_1: 0.9441\n",
      "Epoch 133/200\n",
      "63/63 [==============================] - 0s 356us/step - loss: 0.1696 - accuracy: 0.9415 - precision_1: 0.9613 - recall_1: 0.9255\n",
      "Epoch 134/200\n",
      "63/63 [==============================] - 0s 335us/step - loss: 0.2055 - accuracy: 0.9362 - precision_1: 0.9475 - recall_1: 0.9122\n",
      "Epoch 135/200\n",
      "63/63 [==============================] - 0s 350us/step - loss: 0.1491 - accuracy: 0.9468 - precision_1: 0.9591 - recall_1: 0.9362\n",
      "Epoch 136/200\n",
      "63/63 [==============================] - 0s 352us/step - loss: 0.2645 - accuracy: 0.9282 - precision_1: 0.9605 - recall_1: 0.9043\n",
      "Epoch 137/200\n",
      "63/63 [==============================] - 0s 337us/step - loss: 0.1632 - accuracy: 0.9495 - precision_1: 0.9614 - recall_1: 0.9282\n",
      "Epoch 138/200\n",
      "63/63 [==============================] - 0s 352us/step - loss: 0.2364 - accuracy: 0.9309 - precision_1: 0.9503 - recall_1: 0.9149\n",
      "Epoch 139/200\n",
      "63/63 [==============================] - 0s 351us/step - loss: 0.1459 - accuracy: 0.9548 - precision_1: 0.9668 - recall_1: 0.9282\n",
      "Epoch 140/200\n",
      "63/63 [==============================] - 0s 341us/step - loss: 0.1774 - accuracy: 0.9441 - precision_1: 0.9639 - recall_1: 0.9229\n",
      "Epoch 141/200\n",
      "63/63 [==============================] - 0s 351us/step - loss: 0.1434 - accuracy: 0.9362 - precision_1: 0.9508 - recall_1: 0.9255\n",
      "Epoch 142/200\n",
      "63/63 [==============================] - 0s 354us/step - loss: 0.2284 - accuracy: 0.9282 - precision_1: 0.9520 - recall_1: 0.8963\n",
      "Epoch 143/200\n",
      "63/63 [==============================] - 0s 349us/step - loss: 0.1971 - accuracy: 0.9415 - precision_1: 0.9558 - recall_1: 0.9202\n",
      "Epoch 144/200\n",
      "63/63 [==============================] - 0s 342us/step - loss: 0.2134 - accuracy: 0.9229 - precision_1: 0.9449 - recall_1: 0.9122\n",
      "Epoch 145/200\n",
      "63/63 [==============================] - 0s 348us/step - loss: 0.1354 - accuracy: 0.9441 - precision_1: 0.9620 - recall_1: 0.9415\n",
      "Epoch 146/200\n",
      "63/63 [==============================] - 0s 352us/step - loss: 0.1855 - accuracy: 0.9415 - precision_1: 0.9536 - recall_1: 0.9282\n",
      "Epoch 147/200\n",
      "63/63 [==============================] - 0s 337us/step - loss: 0.1552 - accuracy: 0.9521 - precision_1: 0.9645 - recall_1: 0.9388\n",
      "Epoch 148/200\n",
      "63/63 [==============================] - 0s 353us/step - loss: 0.1934 - accuracy: 0.9309 - precision_1: 0.9503 - recall_1: 0.9149\n",
      "Epoch 149/200\n",
      "63/63 [==============================] - 0s 358us/step - loss: 0.1398 - accuracy: 0.9495 - precision_1: 0.9725 - recall_1: 0.9415\n",
      "Epoch 150/200\n",
      "63/63 [==============================] - 0s 342us/step - loss: 0.1599 - accuracy: 0.9388 - precision_1: 0.9644 - recall_1: 0.9362\n",
      "Epoch 151/200\n",
      "63/63 [==============================] - 0s 357us/step - loss: 0.2502 - accuracy: 0.9122 - precision_1: 0.9365 - recall_1: 0.9016\n",
      "Epoch 152/200\n",
      "63/63 [==============================] - 0s 355us/step - loss: 0.1547 - accuracy: 0.9574 - precision_1: 0.9722 - recall_1: 0.9309\n",
      "Epoch 153/200\n",
      "63/63 [==============================] - 0s 331us/step - loss: 0.1569 - accuracy: 0.9574 - precision_1: 0.9647 - recall_1: 0.9441\n",
      "Epoch 154/200\n",
      "63/63 [==============================] - 0s 350us/step - loss: 0.1263 - accuracy: 0.9521 - precision_1: 0.9643 - recall_1: 0.9335\n",
      "Epoch 155/200\n",
      "63/63 [==============================] - 0s 348us/step - loss: 0.2091 - accuracy: 0.9362 - precision_1: 0.9475 - recall_1: 0.9122\n",
      "Epoch 156/200\n",
      "63/63 [==============================] - 0s 351us/step - loss: 0.1565 - accuracy: 0.9388 - precision_1: 0.9503 - recall_1: 0.9149\n",
      "Epoch 157/200\n",
      "63/63 [==============================] - 0s 342us/step - loss: 0.1992 - accuracy: 0.9176 - precision_1: 0.9440 - recall_1: 0.8963\n",
      "Epoch 158/200\n",
      "63/63 [==============================] - 0s 355us/step - loss: 0.1830 - accuracy: 0.9362 - precision_1: 0.9475 - recall_1: 0.9122\n",
      "Epoch 159/200\n",
      "63/63 [==============================] - 0s 359us/step - loss: 0.1727 - accuracy: 0.9415 - precision_1: 0.9611 - recall_1: 0.9202\n",
      "Epoch 160/200\n",
      "63/63 [==============================] - 0s 343us/step - loss: 0.2259 - accuracy: 0.9229 - precision_1: 0.9449 - recall_1: 0.9122\n",
      "Epoch 161/200\n",
      "63/63 [==============================] - 0s 366us/step - loss: 0.1855 - accuracy: 0.9495 - precision_1: 0.9665 - recall_1: 0.9202\n",
      "Epoch 162/200\n",
      "63/63 [==============================] - 0s 364us/step - loss: 0.1568 - accuracy: 0.9388 - precision_1: 0.9457 - recall_1: 0.9255\n",
      "Epoch 163/200\n",
      "63/63 [==============================] - 0s 359us/step - loss: 0.1517 - accuracy: 0.9441 - precision_1: 0.9612 - recall_1: 0.9229\n",
      "Epoch 164/200\n",
      "63/63 [==============================] - 0s 342us/step - loss: 0.1345 - accuracy: 0.9548 - precision_1: 0.9591 - recall_1: 0.9362\n",
      "Epoch 165/200\n",
      "63/63 [==============================] - 0s 376us/step - loss: 0.1207 - accuracy: 0.9521 - precision_1: 0.9674 - recall_1: 0.9468\n",
      "Epoch 166/200\n",
      "63/63 [==============================] - 0s 578us/step - loss: 0.1730 - accuracy: 0.9468 - precision_1: 0.9719 - recall_1: 0.9202\n",
      "Epoch 167/200\n",
      "63/63 [==============================] - 0s 380us/step - loss: 0.1786 - accuracy: 0.9468 - precision_1: 0.9590 - recall_1: 0.9335\n",
      "Epoch 168/200\n",
      "63/63 [==============================] - 0s 361us/step - loss: 0.1332 - accuracy: 0.9495 - precision_1: 0.9568 - recall_1: 0.9415\n",
      "Epoch 169/200\n",
      "63/63 [==============================] - 0s 344us/step - loss: 0.1813 - accuracy: 0.9415 - precision_1: 0.9559 - recall_1: 0.9229\n",
      "Epoch 170/200\n",
      "63/63 [==============================] - 0s 347us/step - loss: 0.1555 - accuracy: 0.9574 - precision_1: 0.9726 - recall_1: 0.9441\n",
      "Epoch 171/200\n",
      "63/63 [==============================] - 0s 357us/step - loss: 0.2444 - accuracy: 0.9176 - precision_1: 0.9319 - recall_1: 0.9096\n",
      "Epoch 172/200\n",
      "63/63 [==============================] - 0s 343us/step - loss: 0.2415 - accuracy: 0.9122 - precision_1: 0.9317 - recall_1: 0.9069\n",
      "Epoch 173/200\n",
      "63/63 [==============================] - 0s 355us/step - loss: 0.2331 - accuracy: 0.9441 - precision_1: 0.9639 - recall_1: 0.9229\n",
      "Epoch 174/200\n",
      "63/63 [==============================] - 0s 353us/step - loss: 0.1895 - accuracy: 0.9282 - precision_1: 0.9505 - recall_1: 0.9202\n",
      "Epoch 175/200\n",
      "63/63 [==============================] - 0s 349us/step - loss: 0.1710 - accuracy: 0.9468 - precision_1: 0.9510 - recall_1: 0.9282\n",
      "Epoch 176/200\n",
      "63/63 [==============================] - 0s 342us/step - loss: 0.1774 - accuracy: 0.9388 - precision_1: 0.9564 - recall_1: 0.9335\n",
      "Epoch 177/200\n",
      "63/63 [==============================] - 0s 348us/step - loss: 0.1615 - accuracy: 0.9521 - precision_1: 0.9643 - recall_1: 0.9335\n",
      "Epoch 178/200\n",
      "63/63 [==============================] - 0s 354us/step - loss: 0.1729 - accuracy: 0.9495 - precision_1: 0.9672 - recall_1: 0.9415\n",
      "Epoch 179/200\n",
      "63/63 [==============================] - 0s 339us/step - loss: 0.1905 - accuracy: 0.9495 - precision_1: 0.9669 - recall_1: 0.9309\n",
      "Epoch 180/200\n",
      "63/63 [==============================] - 0s 354us/step - loss: 0.1454 - accuracy: 0.9628 - precision_1: 0.9677 - recall_1: 0.9574\n",
      "Epoch 181/200\n",
      "63/63 [==============================] - 0s 353us/step - loss: 0.1685 - accuracy: 0.9441 - precision_1: 0.9586 - recall_1: 0.9229\n",
      "Epoch 182/200\n",
      "63/63 [==============================] - 0s 341us/step - loss: 0.1341 - accuracy: 0.9574 - precision_1: 0.9622 - recall_1: 0.9468\n",
      "Epoch 183/200\n",
      "63/63 [==============================] - 0s 352us/step - loss: 0.1112 - accuracy: 0.9574 - precision_1: 0.9704 - recall_1: 0.9574\n",
      "Epoch 184/200\n",
      "63/63 [==============================] - 0s 354us/step - loss: 0.1738 - accuracy: 0.9388 - precision_1: 0.9560 - recall_1: 0.9255\n",
      "Epoch 185/200\n",
      "63/63 [==============================] - 0s 354us/step - loss: 0.1530 - accuracy: 0.9441 - precision_1: 0.9512 - recall_1: 0.9335\n",
      "Epoch 186/200\n",
      "63/63 [==============================] - 0s 338us/step - loss: 0.1712 - accuracy: 0.9441 - precision_1: 0.9672 - recall_1: 0.9415\n",
      "Epoch 187/200\n",
      "63/63 [==============================] - 0s 352us/step - loss: 0.1322 - accuracy: 0.9601 - precision_1: 0.9781 - recall_1: 0.9521\n",
      "Epoch 188/200\n",
      "63/63 [==============================] - 0s 353us/step - loss: 0.1662 - accuracy: 0.9521 - precision_1: 0.9644 - recall_1: 0.9362\n",
      "Epoch 189/200\n",
      "63/63 [==============================] - 0s 338us/step - loss: 0.1677 - accuracy: 0.9388 - precision_1: 0.9488 - recall_1: 0.9362\n",
      "Epoch 190/200\n",
      "63/63 [==============================] - 0s 372us/step - loss: 0.1384 - accuracy: 0.9388 - precision_1: 0.9641 - recall_1: 0.9282\n",
      "Epoch 191/200\n",
      "63/63 [==============================] - 0s 355us/step - loss: 0.1652 - accuracy: 0.9521 - precision_1: 0.9722 - recall_1: 0.9309\n",
      "Epoch 192/200\n",
      "63/63 [==============================] - 0s 341us/step - loss: 0.1202 - accuracy: 0.9495 - precision_1: 0.9674 - recall_1: 0.9468\n",
      "Epoch 193/200\n",
      "63/63 [==============================] - 0s 362us/step - loss: 0.1609 - accuracy: 0.9495 - precision_1: 0.9568 - recall_1: 0.9415\n",
      "Epoch 194/200\n",
      "63/63 [==============================] - 0s 756us/step - loss: 0.1534 - accuracy: 0.9574 - precision_1: 0.9699 - recall_1: 0.9441\n",
      "Epoch 195/200\n",
      "63/63 [==============================] - 0s 355us/step - loss: 0.0797 - accuracy: 0.9734 - precision_1: 0.9810 - recall_1: 0.9601\n",
      "Epoch 196/200\n",
      "63/63 [==============================] - 0s 358us/step - loss: 0.1549 - accuracy: 0.9415 - precision_1: 0.9485 - recall_1: 0.9309\n",
      "Epoch 197/200\n",
      "63/63 [==============================] - 0s 343us/step - loss: 0.1663 - accuracy: 0.9441 - precision_1: 0.9590 - recall_1: 0.9335\n",
      "Epoch 198/200\n",
      "63/63 [==============================] - 0s 366us/step - loss: 0.1485 - accuracy: 0.9548 - precision_1: 0.9595 - recall_1: 0.9441\n",
      "Epoch 199/200\n",
      "63/63 [==============================] - 0s 360us/step - loss: 0.2486 - accuracy: 0.9362 - precision_1: 0.9558 - recall_1: 0.9202\n",
      "Epoch 200/200\n",
      "63/63 [==============================] - 0s 557us/step - loss: 0.2459 - accuracy: 0.9149 - precision_1: 0.9417 - recall_1: 0.9016\n"
     ]
    }
   ],
   "source": [
    "#fitting and saving the model\n",
    "np.random.seed(123)\n",
    "import random as python_random\n",
    "python_random.seed(123)\n",
    "hist = model.fit(np.array(X_train), np.array(y_train), epochs=200, batch_size=6, verbose=1)\n",
    "model.save('my_model.h5', hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "5PeCfN1UlpF4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "JWKPZTPZllbV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "model = load_model('my_model.h5', custom_objects={'F1Score': F1Score})\n",
    "intents = json.loads(open('intents.json').read())\n",
    "words = pickle.load(open('unique_words.pkl', 'rb'))\n",
    "classes = pickle.load(open('classes1.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doğruluk (Accuracy): 0.9378851652145386\n",
      "Kesinlik (Precision): 0.8105263113975525\n",
      "Duyarlılık (Recall): 0.8690476417541504\n",
      "F1 Skoru: 0.7684210538864136\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1_score = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"Doğruluk (Accuracy): {accuracy}\")\n",
    "print(f\"Kesinlik (Precision): {precision}\")\n",
    "print(f\"Duyarlılık (Recall): {recall}\")\n",
    "print(f\"F1 Skoru: {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "YS_qTac6hENU"
   },
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "def bow(sentence, words, show_details=True):\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    bag = [0]*len(words)\n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s:\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print(\"found in bag: %s\" % w)\n",
    "    return(np.array(bag))\n",
    "\n",
    "def predict_class(sentence, model):\n",
    "    p = bow(sentence, words, show_details=False)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    return return_list\n",
    "\n",
    "def getResponse(ints, intents_json):\n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if(i['tag'] == tag):\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    return result\n",
    "\n",
    "def chatbot_response(msg):\n",
    "    ints = predict_class(msg, model)\n",
    "    res = getResponse(ints, intents)\n",
    "    return res\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oUeXWsTxluUK",
    "outputId": "822438af-e664-404c-e308-6e2fa21715c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Delizeroo! How can I help you?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Customer:  Hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n",
      "Chatbot: Good to see you again!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Customer:  How does it work?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "Chatbot: You can order either on the website or by using the Deliveroo app, available on iOS and Android. Simply add your postcode to find all the great restaurants delivering in your area, choose your food and place your order\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Customer:  What if something is wrong with my order?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "Chatbot: We have a dedicated team that looks after your entire Delizeroo experience, from the moment you place an order right through to it arriving with you\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Customer:  Why doesn't Deliveroo accept cash?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "Chatbot: We only take card payments because it lets us provide you with the best possible experience. The minimum order amount can vary depending on which restaurant you’re ordering from\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Customer:  I'm not a new user. Can I get free credit if I sign up?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "Chatbot: Sorry, this offer is only valid for new users of Delizeroo. And don't keep making accounts, we'll know!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Customer:  How can I cancel my order?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "Chatbot: Please provide order number\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Customer:  3435456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "Chatbot: Maintaining decency would be appreciated\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Customer:  My order's late. Where is it?\", \"I would like to receive information about my order\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "Chatbot: Please provide full name and order number\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Customer:  secil, 234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 8ms/step\n",
      "Chatbot: Talk to you later\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to Delizeroo! How can I help you?\")\n",
    "while True:\n",
    "    msg = input(\"Customer: \")\n",
    "    if msg == \"quit\":\n",
    "        break\n",
    "    response = chatbot_response(msg)\n",
    "    print(\"Chatbot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
